{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Maintenance\n",
    "\n",
    "This notebook is part of [*Hands-on Machine Learning for IoT*](https://github.com/pablodecm/datalab_ml_iot) tutorial by Pablo de Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools\n",
    "\n",
    "This notebook will use the following Python 3\n",
    "libraries for data analytics and machine learning:\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- scikit-learn\n",
    "- keras/tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset that will be used, which was published by NASA [[3](#References)],\n",
    "consist on simulated turbojet engine degradation\n",
    "under different combinations of operational conditions.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/airbus-turbofan.jpg\" height=\"50%\" style=\"max-width: 50%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget https://ti.arc.nasa.gov/c/6/ -O data/CMAPSSData.zip\n",
    "!unzip -o data/CMAPSSData.zip -d data\n",
    "!ls -lrth data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cat data/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# train and test data are simple space separated values\n",
    "!head -5 data/train_FD001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the test truth is given as the number of steps to failure\n",
    "# for each engine run in the test set (100 in total)\n",
    "!head -5 data/RUL_FD001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load data (only gonna use FD001 dataset)\n",
    "train_df = pd.read_csv('data/train_FD001.txt', sep=\" \", header=None)\n",
    "test_df  = pd.read_csv('data/test_FD001.txt', sep=\" \", header=None)\n",
    "print(\"train shape: \", train_df.shape, \"test shape: \", test_df.shape)\n",
    "# lets have a look at basic descriptive statistics\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we will remove columns 26 and 27 because of the NaNs\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "print(\"train shape: \", train_df.shape, \"test shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the files did not contain headers\n",
    "# we can create them based on the documentation\n",
    "target_var = ['target_RUL']\n",
    "index_columns_names =  [\"UnitNumber\",\"Cycle\"]\n",
    "op_settings_columns = [\"Op_Setting_\"+str(i) for i in range(1,4)]\n",
    "sensor_columns =[\"Sensor_\"+str(i) for i in range(1,22)]\n",
    "column_names = index_columns_names + op_settings_columns + sensor_columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# name columns\n",
    "train_df.columns = column_names\n",
    "test_df.columns = column_names\n",
    "\n",
    "# now the dataset looks better, e.g. the first unit\n",
    "train_df[train_df.UnitNumber == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remaining Useful Life (RUL)\n",
    "\n",
    "The training data consists time-series for the engine sensors\n",
    "for each cycle (i.e. timestep) until failure which happens\n",
    "after the last time step.\n",
    "\n",
    "Thus, the Remaining Useful Life (RUL), i.e. time until the\n",
    "engine breaks, can be calculated based on the maximum cycle\n",
    "of each unit present in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# find the last cycle per unit number\n",
    "max_cycle = train_df.groupby('UnitNumber')['Cycle'].max().reset_index()\n",
    "max_cycle.columns = ['UnitNumber', 'MaxOfCycle']\n",
    "# merge the max cycle back into the original frame\n",
    "train_df = train_df.merge(max_cycle, left_on='UnitNumber', right_on='UnitNumber', how='inner')\n",
    "# calculate RUL for each row\n",
    "target_RUL = train_df[\"MaxOfCycle\"] - train_df[\"Cycle\"]\n",
    "# add columns and remove MaxOfCycle\n",
    "train_df[\"target_RUL\"] = target_RUL\n",
    "train_df = train_df.drop(\"MaxOfCycle\", axis=1)\n",
    "# check that it worked for unit 1\n",
    "train_df[train_df.UnitNumber == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The test data does not correspond to running until failure\n",
    "as discussed in the dataset documentation, the RUL at the last\n",
    "step is instead provided on an additional file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get truth RUL\n",
    "truth_df = pd.read_csv('data/RUL_FD001.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "# UnitNumber based\n",
    "truth_df.columns = [\"RUL_after_last\"]\n",
    "truth_df['UnitNumber'] =  truth_df.index + 1\n",
    "# find the last cycle per unit number in test set\n",
    "max_cycle = test_df.groupby('UnitNumber')['Cycle'].max().reset_index()\n",
    "max_cycle.columns = ['UnitNumber', 'MaxOfCycle']\n",
    "max_cycle['MaxOfCycle'] = max_cycle['MaxOfCycle'] + truth_df[\"RUL_after_last\"]\n",
    "# merge the max cycle back into the original frame\n",
    "test_df = test_df.merge(max_cycle, left_on='UnitNumber', right_on='UnitNumber', how='inner')\n",
    "# calculate RUL for each row\n",
    "target_RUL = test_df[\"MaxOfCycle\"] - test_df[\"Cycle\"]\n",
    "# add columns and remove MaxOfCycle\n",
    "test_df[\"target_RUL\"] = target_RUL\n",
    "test_df = test_df.drop(\"MaxOfCycle\", axis=1)\n",
    "# check that it worked for unit 1\n",
    "test_df[test_df.UnitNumber == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Exploration\n",
    "\n",
    "Independently on the chosen problem, it is always recommended\n",
    "to interactively explore the variables to be considered\n",
    "in the predictive modelling problem.\n",
    "\n",
    "It is important to not only consider the data available\n",
    "in the training set but also that expected in the test\n",
    "or production environment. Alternatively we can\n",
    "incur in:\n",
    "- **target leakage**: use information that has predictive power\n",
    "as input of the model during training but will no be available\n",
    "in production or for the real data.\n",
    "- **domain mismatch**: if the training and test/production data\n",
    "are different the trained models might not perform well in\n",
    "the real word scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we will consider all features except the UnitNumber and the target\n",
    "basic_features = train_df.columns.difference([\"UnitNumber\",\"target_RUL\"])\n",
    "print(\"basic_features: \",basic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Transformations and Engineering\n",
    "\n",
    "Another important step, particularly when dealing with\n",
    "most techniques other than Deep Learning, is *feature\n",
    "preprocessing/scaling* and *feature engineering*.\n",
    "\n",
    "Feature preprocessing/scaling can facilitate model training by\n",
    "scaling the features to dimensionless values based on the properties\n",
    "of the dataset.\n",
    "\n",
    "Feature engineering, the definition of new variables based\n",
    "on those available, is particularly important for time-series\n",
    "data when we want to\n",
    "make a prediction for each timestep as is the case for all the\n",
    "problems considered in this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# we will use the Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "features = basic_features\n",
    "X_unscaled = train_df[features].astype('float64')\n",
    "X = pd.DataFrame(scaler.fit_transform(X_unscaled),\n",
    "                 columns = features,\n",
    "                 index = train_df.index)\n",
    "y = train_df[\"target_RUL\"]\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_unscaled = test_df[features].astype('float64')\n",
    "X_test = pd.DataFrame(scaler.transform(X_test_unscaled),\n",
    "                      columns = features,\n",
    "                      index = test_df.index)\n",
    "y_test = test_df[\"target_RUL\"]\n",
    "\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RUL Prediction as a Regression Task\n",
    "\n",
    "**How many more cycles an in-service engine will last before it fails?**\n",
    "\n",
    "The task of predicting the Remaining Useful Life (RUL)\n",
    "can be casted as a regression task in the context of machine\n",
    "learning. RUL can also be referred as Time to Failure (TTF).\n",
    "\n",
    "\n",
    "The goal in a regression problem is to find a function\n",
    "$f_R(\\boldsymbol{x})$ that approximates the true target $y$. The\n",
    "target in this problem will be the RUL, while $\\boldsymbol{x}$\n",
    "will be the input features (e.g. sensor readings).\n",
    "\n",
    "To measure the goodness of our regression function, we need\n",
    "to define and score function, which will be also the loss\n",
    "function for some of the machine learning techniques considered.\n",
    "We will be considering the mean squared error:\n",
    "$$ \\textrm{MSE} = \\sum_{i=1}^{n} (y - f_R(\\boldsymbol{x}))^2$$\n",
    "which is one of the most common losses used for regression,\n",
    "depending on the problem alternative loss definitions might\n",
    "be more beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline and Feature Importance\n",
    "\n",
    "Sometimes is useful to train a simple model for the task at hand to\n",
    "get a baseline performance. A random forest has the advantage that\n",
    "it can also provide a list of the relative importance of the\n",
    "different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rf = ensemble.RandomForestRegressor()\n",
    "simple_rf = ensemble.RandomForestRegressor(n_estimators = 200, max_depth = 15)\n",
    "simple_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = simple_rf.predict(X)\n",
    "print(\"[Train] Simple RF Mean Squared Error: \", mean_squared_error(y, y_pred))\n",
    "print(\"[Train] Simple RF Mean Absolute Error: \", mean_absolute_error(y, y_pred))\n",
    "print(\"[Train] Simple RF r-squared: \", r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = simple_rf.predict(X_test)\n",
    "print(\"[Test] Simple RF Mean Squared Error: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"[Test] Simple RF Mean Absolute Error: \", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"[Test] Simple RF r-squared: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "importances = simple_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns    \n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "plt.title(\"Feature ranking\", fontsize = 20)\n",
    "plt.bar(range(X.shape[1]), importances[indices], color=\"b\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices) #feature_names, rotation='vertical')\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylabel(\"importance\", fontsize = 18)\n",
    "plt.xlabel(\"index of the feature\", fontsize = 18)\n",
    "plt.show()\n",
    "# list feature importance\n",
    "important_features = pd.Series(data=single_rf.feature_importances_,index=X.columns)\n",
    "important_features.sort_values(ascending=False,inplace=True)\n",
    "print(important_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation and Hyper-Parameters\n",
    "\n",
    "By means of cross validation and hyper-parameter search, we\n",
    "can try to obtain a better regression model based on RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "\n",
    "rf = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# to avoid having same UnitNumber in both sets\n",
    "cv = GroupKFold(5)\n",
    "\n",
    "param_grid = { \"min_samples_leaf\" : [2, 10, 25, 50, 100],\n",
    "               \"max_depth\" : [7, 8, 9, 10, 11, 12]}\n",
    "\n",
    "optimized_rf = GridSearchCV(estimator=rf,\n",
    "                            cv = cv,\n",
    "                            param_grid=param_grid,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            verbose = 1,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "optimized_rf.fit(X, y, groups = train_df.UnitNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = optimized_rf.predict(X_test)\n",
    "print(\"[Test] Optimized RF Mean Squared Error: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"[Test] Optimized RF Mean Absolute Error: \", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"[Test] Optimized RF r-squared: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Models: Gradient Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Train Another Cross Validated Model\n",
    "\n",
    "Based on the previous examples, use cross validation to find a good\n",
    "set of hyper-parameters and benchmark on the test dataset another scikit-learn regression model ([see documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)), for example choose between:\n",
    "1. `sklearn.svm.SVR`: [Epsilon-Support Vector Regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)\n",
    "2. `sklearn.neural_network.MLPRegressor` : [Multilayer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)\n",
    "3. **Advanced Track**: use `GradientBoostingRegressor` but add some engineering features in order to try to improve the previous result (e.g. moving averages and standard deviations of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for the exercise solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network Model for Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predict Failures using Classification\n",
    "\n",
    "**Will the unit fail within a certain time-frame (i.e. number of cycles)?**\n",
    "\n",
    "This can be though of a classification problem,\n",
    "where the boolean target is whether the unit will fail\n",
    "within the next $n$ timesteps (e.g. 15 cycles).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Recurrent Neural Network Model for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for the exercise solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "This notebook is heavily based on these resources on the topic:\n",
    "\n",
    "- [1] [*Predictive Maintenance Template*](https://gallery.azure.ai/Collection/Predictive-Maintenance-Template-3)  by Microsoft Azure ML Team \n",
    "- [2] [*Predictive Maintenance using LSTM*](https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM) by Umberto Griffo\n",
    "- [3] [*Predictive Maintenance ML (IOT)*](https://www.kaggle.com/billstuart/predictive-maintenance-ml-iiot) by Bill Stuart\n",
    "\n",
    "The dataset used was provided by NASA:\n",
    "\n",
    "- [4] A. Saxena and K. Goebel (2008). [*Turbofan Engine Degradation Simulation Data Set*](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan), NASA Ames Prognostics Data Repository, NASA Ames Research Center, Moffett Field, CA\n",
    "\n",
    "\n",
    "Other resources and tutorials on Predictive Maintenance:\n",
    "\n",
    "- [5] [*Predictive Maintenance Modelling Guide*](https://gallery.azure.ai/Collection/Predictive-Maintenance-Implementation-Guide-1) by Fidan Boyly Uz\n",
    "\n",
    "Two good really good posts on the concept and usefulness of Recurrent Neural Networks for sequence data:\n",
    "\n",
    "- [6] [*The Unreasonable Effectiveness of Recurrent Neural Networks*](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy\n",
    "\n",
    "- [7] [*Understanding LSTM Networks*](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah\n",
    "\n",
    "This recent papers (with code) use this dataset in combination\n",
    "with more advanced Deep Learning architectures and data augmentation\n",
    "to achieve state of the art (SOTA):\n",
    "\n",
    "- [8] S. Theng et al. [*Long short-term\n",
    "memory network for remaining useful life estimation*](https://ieeexplore.ieee.org/document/7998311) in Proc. IEEE International Conference on Prognostics and Health\n",
    "\n",
    "- [9] L. Jayasinghe et al. [*Temporal Convolutional Memory Networks for\n",
    "Remaining Useful Life Estimation of Industrial Machinery*](https://github.com/LahiruJayasinghe/RUL-Net) in IEEE International Conference on Industrial Technology (ICIT2019)\n",
    "\n",
    "Not many books on this topic, there is a book on ML for IOT (free 1 month subscription online), yet the\n",
    "advanced ML chapter do not include specific IoT applications:\n",
    "\n",
    "- [10] [Hands-On Artificial Intelligence for IoT](https://www.packtpub.com/big-data-and-business-intelligence/hands-artificial-intelligence-iot?utm_source=github&utm_medium=repository&utm_campaign=9781788836067) by Amita Kapoor (2019)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
