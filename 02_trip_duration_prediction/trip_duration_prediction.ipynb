{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trip Duration Prediction\n",
    "\n",
    "This notebook is part of [*Hands-on Machine Learning for IoT*](https://github.com/pablodecm/datalab_ml_iot) tutorial by Pablo de Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools\n",
    "\n",
    "This notebook will use the following Python 3\n",
    "libraries for data analytics and machine learning:\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib/seaborn\n",
    "- scikit-learn\n",
    "- xgboost\n",
    "- leaflet/folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "In this notebook, we are gonna be using a large dataset from\n",
    "the [Kaggle New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration)\n",
    "challenge, which corresponds to real taxi trips data in the city\n",
    "of New York within the year 2016.\n",
    "\n",
    "The main task is the prediction of the trip duration given the features, but it\n",
    "is a really good dataset for exploratory data analysis and applying some\n",
    "tricks for dealing with location and temporal data in cities.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/kaggle_trip_duration.png\" height=\"50%\" style=\"max-width: 50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data fields\n",
    "\n",
    "Here is a list and description of all the provided items for each trip:\n",
    "- **id** - a unique identifier for each trip\n",
    "- **vendor_id** - a code indicating the provider associated with the trip record\n",
    "- **pickup_datetime** - date and time when the meter was engaged\n",
    "- **dropoff_datetime** - date and time when the meter was disengaged\n",
    "- **passenger_count** - the number of passengers in the vehicle (driver entered value)\n",
    "- **pickup_longitude** - the longitude where the meter was engaged\n",
    "- **pickup_latitude** - the latitude where the meter was engaged\n",
    "- **dropoff_longitude** - the longitude where the meter was disengaged\n",
    "- **dropoff_latitude** - the latitude where the meter was disengaged\n",
    "- **store_and_fwd_flag** - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server -  Y=store and forward; N=not a store and forward trip\n",
    "- **trip_duration** - duration of the trip in seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Download from Kaggle\n",
    "\n",
    "In order to download datasets from Kaggle with can use the\n",
    "official CLI interface, but it requires you to have an account\n",
    "and to get an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!mkdir $HOME/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# run if in Google Colab to setup your Kaggle API Key\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "username = getpass.getpass('username')\n",
    "api_key = getpass.getpass('Kaggle API key')\n",
    "\n",
    "token = {\"username\": username,\"key\":api_key}\n",
    "with open('kaggle.json', 'w') as file:\n",
    "    json.dump(token, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!mv kaggle.json $HOME/.kaggle/kaggle.json\n",
    "!chmod 600 $HOME/.kaggle/kaggle.json\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!cd data; kaggle competitions download -c nyc-taxi-trip-duration; cd ..\n",
    "!cd data; unzip train.zip; unzip test.zip; cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "print(\"train shape: \", train_df.shape, \"test shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warming up Exercise\n",
    "\n",
    "Check that id is unique and train and test set are distinct.\n",
    "\n",
    "*Hint: Look up `DataFrame.nunique` and `np.intersect1d` in their respective documentations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Datasets\n",
    "\n",
    "In real-world scenarios, there is often something to gain\n",
    "by combining data from different sources that might be informative\n",
    "for the task.\n",
    "\n",
    "In the case of car trip durations, traffic routes and weather\n",
    "are quite relevant and were allowed in the competition.\n",
    "Participants have\n",
    "curated two datasets that might be of use here:\n",
    "- Traffic route details using the [Open Source Routing Machine OSRM tool](http://project-osrm.org/)\n",
    "- Weather during the period considered\n",
    "Part of the information of the former will be used while\n",
    "the use of the later if left for future extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic route from OSRM\n",
    "!cd data; kaggle datasets download oscarleo/new-york-city-taxi-with-osrm; cd ..\n",
    "!mkdir data/osrm\n",
    "!unzip -o data/new-york-city-taxi-with-osrm.zip -d data/osrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather data\n",
    "!cd data; kaggle datasets download mathijs/weather-data-in-new-york-city-2016; cd ..\n",
    "!mkdir data/weather\n",
    "!unzip -o data/weather-data-in-new-york-city-2016.zip -d data/weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some columns to the train and test data\n",
    "cols_osrm = ['id', 'total_distance', 'total_travel_time',  'number_of_steps']\n",
    "fr1 = pd.read_csv('data/osrm/fastest_routes_train_part_1.csv', usecols=cols_osrm)\n",
    "fr2 = pd.read_csv('data/osrm/fastest_routes_train_part_2.csv', usecols=cols_osrm)\n",
    "test_street_info = pd.read_csv('data/osrm/fastest_routes_test.csv',\n",
    "                               usecols=cols_osrm)\n",
    "train_street_info = pd.concat((fr1, fr2))\n",
    "train_df = train_df.merge(train_street_info, how='left', on='id')\n",
    "test_df = test_df.merge(test_street_info, how='left', on='id')\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "It is good to get used to the properties data in an interactive\n",
    "manner before carrying out any model building, which is often done\n",
    "by some basic visualization and summary descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# really long trips present\n",
    "3.526282e+06/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can remove outliers to simplify the analysis and make it\n",
    "more robust, e.g. we can safely remove trips with a duration\n",
    "further away from the mean than 3 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(train_df['trip_duration'])\n",
    "s = np.std(train_df['trip_duration'])\n",
    "# filter \n",
    "filter_duration = ((train_df['trip_duration'] <= m + 3*s) &\n",
    "                   (train_df['trip_duration'] >= m - 3*s))\n",
    "(~filter_duration).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, the bounding box limits of the city of New York can\n",
    "be easily checked and used for limiting the exploration of data\n",
    "to trips which were started or finished within the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_long_border = (-74.03, -73.75)\n",
    "city_lat_border = (40.63, 40.85)\n",
    "filter_location = ((train_df['pickup_longitude'] <= city_long_border[1]) &\n",
    "                   (train_df['pickup_longitude'] >= city_long_border[0]) &\n",
    "                   (train_df['pickup_latitude'] <= city_lat_border[1]) &\n",
    "                   (train_df['pickup_latitude'] >= city_lat_border[0]) &\n",
    "                   (train_df['dropoff_longitude'] <= city_long_border[1]) &\n",
    "                   (train_df['dropoff_longitude'] >= city_long_border[0]) &\n",
    "                   (train_df['dropoff_latitude'] <= city_lat_border[1]) &\n",
    "                   (train_df['dropoff_latitude'] >= city_lat_border[0]))\n",
    "                                 \n",
    "(~filter_location).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you have not heard of it\n",
    "# this is a very useful DataFrame function\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(train_df.loc[filter_duration,'trip_duration'].values, bins=100)\n",
    "ax.set_xlabel('trip_duration (seconds)')\n",
    "ax.set_ylabel('number of train records');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['log_trip_duration'] = np.log(train_df['trip_duration'].values + 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(train_df['log_trip_duration'].values, bins=100)\n",
    "ax.set_xlabel('log(trip_duration)')\n",
    "ax.set_ylabel('number of train records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = train_df[filter_duration].groupby('passenger_count')['trip_duration'].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.set_ylim(ymax=1100)\n",
    "plt.title('passenger count')\n",
    "plt.ylabel('Time in Seconds')\n",
    "sns.barplot(pc.index,pc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pickup_datetime'] = pd.to_datetime(train_df.pickup_datetime)\n",
    "test_df['pickup_datetime'] = pd.to_datetime(test_df.pickup_datetime)\n",
    "train_df.loc[:, 'pickup_date'] = train_df['pickup_datetime'].dt.date\n",
    "test_df.loc[:, 'pickup_date'] = test_df['pickup_datetime'].dt.date\n",
    "train_df['dropoff_datetime'] = pd.to_datetime(train_df.dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_df.groupby('pickup_date').count()[['id']],\n",
    "         'o-', label='train')\n",
    "ax.plot(test_df.groupby('pickup_date').count()[['id']],\n",
    "         'o-', label='test_df')\n",
    "ax.set_title('Trips over Time.')\n",
    "ax.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_long_border = (-74.03, -73.75)\n",
    "city_lat_border = (40.63, 40.85)\n",
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].scatter(train_df['pickup_longitude'].values[:100000], train_df['pickup_latitude'].values[:100000],\n",
    "              color='blue', s=1, label='train', alpha=0.1)\n",
    "ax[1].scatter(test_df['pickup_longitude'].values[:100000], test_df['pickup_latitude'].values[:100000],\n",
    "              color='green', s=1, label='test_df', alpha=0.1)\n",
    "fig.suptitle('Train and test area complete overlap.')\n",
    "ax[0].legend(loc=0)\n",
    "ax[0].set_ylabel('latitude')\n",
    "ax[0].set_xlabel('longitude')\n",
    "ax[1].set_xlabel('longitude')\n",
    "ax[1].legend(loc=0)\n",
    "plt.ylim(city_lat_border)\n",
    "plt.xlim(city_long_border)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium: Interactive Maps in the Jupyter Notebook\n",
    "\n",
    "\n",
    "Folium is a flexible Python library that can be used to work with interactive Leaflet.js maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = (43.471198, -3.801362)\n",
    "m = folium.Map(location=location,\n",
    "               zoom_start=13)\n",
    "\n",
    "popup = '<b>Us!</b>'\n",
    "folium.Marker(location, popup=popup).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folium Exercise\n",
    "\n",
    "In addition of New York State, where else where New York\n",
    "taxis picked up? Represent some of far the location outliers\n",
    "(remember the `~filter_location` variable) in a Folium map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here the solution of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More Advanced Feature Engineering\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "In some cases, PCA over some of the features can be used to\n",
    "obtain transformed features that can be used more efficiently\n",
    "for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# fit PCA\n",
    "coords = np.vstack((train_df[['pickup_latitude',\n",
    "                           'pickup_longitude']].values,\n",
    "                    train_df[['dropoff_latitude',\n",
    "                           'dropoff_longitude']].values,\n",
    "                    test_df[['pickup_latitude',\n",
    "                          'pickup_longitude']].values,\n",
    "                    test_df[['dropoff_latitude',\n",
    "                          'dropoff_longitude']].values))\n",
    "\n",
    "pca = PCA().fit(coords)\n",
    "\n",
    "# add as new features\n",
    "train_df['pickup_pca0'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "train_df['pickup_pca1'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:, 1]\n",
    "train_df['dropoff_pca0'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "train_df['dropoff_pca1'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n",
    "test_df['pickup_pca0'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "test_df['pickup_pca1'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:, 1]\n",
    "test_df['dropoff_pca0'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "test_df['dropoff_pca1'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Date extraction\n",
    "\n",
    "Get hour of the day, day of the week, day of the month\n",
    "and month number to better encode the date. These will\n",
    "much better represent the periodicity of the traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Month'] = train_df['pickup_datetime'].dt.month\n",
    "test_df['Month'] = test_df['pickup_datetime'].dt.month\n",
    "train_df['DayofMonth'] = train_df['pickup_datetime'].dt.day\n",
    "test_df['DayofMonth'] = test_df['pickup_datetime'].dt.day\n",
    "train_df['Hour'] = train_df['pickup_datetime'].dt.hour\n",
    "test_df['Hour'] = test_df['pickup_datetime'].dt.hour\n",
    "train_df['dayofweek'] = train_df['pickup_datetime'].dt.dayofweek\n",
    "test_df['dayofweek'] = test_df['pickup_datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indicator Variables\n",
    "\n",
    "Categorical data should cannot be used as is within most machine\n",
    "learning techniques. The `pd.get_dummies` function can facilitate\n",
    "the transformation to dummy/indicator variables (also referred as\n",
    "one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# for example this is the case for the vendor id\n",
    "vendor_train = pd.get_dummies(train_df['vendor_id'], prefix='vi', prefix_sep='_')\n",
    "vendor_test = pd.get_dummies(test_df['vendor_id'], prefix='vi', prefix_sep='_')\n",
    "# store_and_fwd_flag\n",
    "store_and_fwd_flag_train = pd.get_dummies(train_df['store_and_fwd_flag'], prefix='sf', prefix_sep='_')\n",
    "store_and_fwd_flag_test = pd.get_dummies(test_df['store_and_fwd_flag'], prefix='sf', prefix_sep='_')\n",
    "# and passenger_count\n",
    "passenger_count_train = pd.get_dummies(train_df['passenger_count'], prefix='pc', prefix_sep='_')\n",
    "passenger_count_test = pd.get_dummies(test_df['passenger_count'], prefix='pc', prefix_sep='_')\n",
    "# remove some columns so train and test have same shape\n",
    "passenger_count_train.drop([\"pc_7\",\"pc_8\",\"pc_9\"],axis=1,inplace=True)\n",
    "passenger_count_test.drop([\"pc_9\"],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we can do the same for the  time categoricals\n",
    "month_train = pd.get_dummies(train_df['Month'], prefix='m', prefix_sep='_')\n",
    "month_test = pd.get_dummies(test_df['Month'], prefix='m', prefix_sep='_')\n",
    "dom_train = pd.get_dummies(train_df['DayofMonth'], prefix='dom', prefix_sep='_')\n",
    "dom_test = pd.get_dummies(test_df['DayofMonth'], prefix='dom', prefix_sep='_')\n",
    "hour_train = pd.get_dummies(train_df['Hour'], prefix='h', prefix_sep='_')\n",
    "hour_test = pd.get_dummies(test_df['Hour'], prefix='h', prefix_sep='_')\n",
    "dow_train = pd.get_dummies(train_df['dayofweek'], prefix='dow', prefix_sep='_')\n",
    "dow_test = pd.get_dummies(test_df['dayofweek'], prefix='dow', prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# remove features that will not be available for test\n",
    "# or for which the indicator function has been computed\n",
    "remove_cols = ['id','vendor_id','passenger_count','store_and_fwd_flag',\n",
    "               'Month','DayofMonth','Hour','dayofweek',\n",
    "                'pickup_datetime','pickup_date',\n",
    "               'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']\n",
    "train_fea = train_df.drop(remove_cols,axis=1)\n",
    "train_fea = train_fea.drop([\"dropoff_datetime\",\"trip_duration\"], axis=1)\n",
    "test_fea = test_df.drop(remove_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_all_fea = pd.concat([train_fea,\n",
    "                          vendor_train,\n",
    "                          passenger_count_train,\n",
    "                          store_and_fwd_flag_train,\n",
    "                          month_train,\n",
    "                          dom_train,\n",
    "                          hour_test,\n",
    "                          dow_train,\n",
    "                          train_df[[\"log_trip_duration\"]]], axis=1)\n",
    "test_all_fea = pd.concat([test_fea, \n",
    "                         vendor_test,\n",
    "                         passenger_count_test,\n",
    "                         store_and_fwd_flag_test,\n",
    "                         month_test,\n",
    "                         dom_test,\n",
    "                         hour_test,\n",
    "                         dow_test], axis=1)\n",
    "print(train_all_fea.shape, test_all_fea.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost Training\n",
    "\n",
    "Now that we have preprocess and engineer the model features\n",
    "we can train a Gradient Boosting regression model to predict\n",
    "the trip duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# we will only consider 100000 examples to speed up training\n",
    "n_samples = 100000\n",
    "train, valid = train_test_split(train_all_fea[0:n_samples], test_size = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['log_trip_duration'], axis=1)\n",
    "y_train = train[\"log_trip_duration\"]\n",
    "X_valid = valid.drop(['log_trip_duration'], axis=1)\n",
    "y_valid = valid[\"log_trip_duration\"]\n",
    "\n",
    "y_valid = y_valid.reset_index().drop('index',axis = 1)\n",
    "y_train = y_train.reset_index().drop('index',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xgb_pars = {'min_child_weight': 1, 'eta': 0.5,\n",
    "            'colsample_bytree': 0.9,  'max_depth': 6,\n",
    "            'subsample': 0.9, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n",
    "\n",
    "model = xgb.train(xgb_pars, dtrain, 10, watchlist, early_stopping_rounds=2,\n",
    "      maximize=False, verbose_eval=1)\n",
    "print('Modeling RMSLE %.5f' % model.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Importances\n",
    "\n",
    "XGBoost also provides a tool to obtain feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model, max_num_features=28, height=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise: Design a Better Model\n",
    "\n",
    "Use XGBoost GridScan or manually change some of the hyper-parameters\n",
    "in order to obtain better RMSLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train another model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "The [top Kaggle Kernels](https://www.kaggle.com/c/nyc-taxi-trip-duration/kernels) (executable environments to similar to Google Collab but aimed for competitions) of the New York City Taxi Trip Duration Playground competition are really good. In particular,\n",
    "this notebook is heavily based on:\n",
    "- [1] [Strength of visualization-python visuals tutorial](https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial) by BuryBuryZymon\n",
    "- [2] [From EDA to the Top (LB 0.367)](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367) by beluga\n",
    "- [3] [NYCT - from A to Z with XGBoost (Tutorial)](https://www.kaggle.com/karelrv/nyct-from-a-to-z-with-xgboost-tutorial) by KarelVerhoeven\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
